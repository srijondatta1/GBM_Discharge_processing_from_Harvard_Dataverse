{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a5d47b-eca9-4e34-aaa6-982579f05373",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ================================================================================================\n",
    "#   HARVARD DATAVERSE DATASET DOWNLOADER (Filtered by Keyword)\n",
    "# ================================================================================================\n",
    "#\n",
    "#   ❖ PURPOSE:\n",
    "#     Downloads only files containing a specific keyword (e.g., \"ERA5L\") from a Harvard Dataverse dataset.\n",
    "#\n",
    "#   ❖ FEATURES:\n",
    "#     - Keyword-based filtering (case-insensitive)\n",
    "#     - Resumable downloads with HTTP Range support\n",
    "#     - Automatic retry on transient errors\n",
    "#     - Clean, single-line progress bar with logical output\n",
    "#     - Professional-style status notifications\n",
    "#\n",
    "# ================================================================================================\n",
    "\n",
    "\n",
    "# STEP 1: IMPORT DEPENDENCIES\n",
    "# ---------------------------\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "from urllib.parse import quote\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "\n",
    "# STEP 2: USER-EDITABLE OPTIONS\n",
    "# -----------------------------\n",
    "# ❖ Customize these variables to suit your dataset and local storage preferences.\n",
    "dataset_doi        = \"doi:10.7910/DVN/V2C6G2\"  # Target dataset DOI\n",
    "download_directory = \"Observed_daily_dscharge/Raw_netcdf\"\n",
    "keyword_filter     = \"ERA5L\"                   # Keyword to filter filenames (case-insensitive)\n",
    "\n",
    "\n",
    "# STEP 3: CONFIGURE RESILIENT HTTP SESSION\n",
    "# ----------------------------------------\n",
    "\n",
    "def configure_session():\n",
    "    print(\"\\n========== STEP 3: CONFIGURING HTTP SESSION ==========\\n\")\n",
    "    print(\">> Subprocess: Setting up retry strategy for network resilience\\n\")\n",
    "\n",
    "    session = requests.Session()\n",
    "    retry_strategy = Retry(\n",
    "        total=5,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        backoff_factor=1\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.mount(\"http://\", adapter)\n",
    "\n",
    "    print(\">> ✅ Session configured to retry up to 5 times on transient errors.\\n\")\n",
    "    return session\n",
    "\n",
    "\n",
    "# STEP 4: RETRIEVE AND FILTER FILES BY KEYWORD (WITH DEBUGGING)\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "def get_filtered_files(persistent_id, session, keyword=\"ERA5L\"):\n",
    "    print(\"\\n========== STEP 4: RETRIEVING AND FILTERING FILES ==========\\n\")\n",
    "    print(\">> Subprocess: Accessing Dataverse API and filtering by keyword\\n\")\n",
    "\n",
    "    try:\n",
    "        encoded_doi = quote(persistent_id, safe='')\n",
    "        files_api_url = f\"https://dataverse.harvard.edu/api/datasets/:persistentId/versions/:latest/files?persistentId={encoded_doi}\"\n",
    "        print(f\">> Requesting file list for DOI: {persistent_id}\")\n",
    "        print(f\">> API Endpoint: {files_api_url}\\n\")\n",
    "\n",
    "        response = session.get(files_api_url, timeout=90)\n",
    "\n",
    "        # NEW: Print HTTP status code and partial response for debugging\n",
    "        print(f\">> HTTP Status Code: {response.status_code}\")\n",
    "        print(f\">> Raw Response Preview:\\n{response.text[:500]}\\n\")\n",
    "\n",
    "        response.raise_for_status()\n",
    "\n",
    "        try:\n",
    "            json_data = response.json()\n",
    "            files_data = json_data.get('data', [])\n",
    "        except ValueError:\n",
    "            print(\">> ❌ ERROR: Response is not valid JSON.\")\n",
    "            print(\">> Raw response:\\n\", response.text[:500])\n",
    "            return []\n",
    "\n",
    "        if not files_data:\n",
    "            print(\">> ❌ No files found in the dataset.\\n\")\n",
    "            return []\n",
    "\n",
    "        # Case-insensitive keyword filtering\n",
    "        filtered_files = [\n",
    "            f for f in files_data\n",
    "            if keyword.lower() in f['dataFile']['filename'].lower()\n",
    "        ]\n",
    "\n",
    "        print(f\">> ✅ Found {len(filtered_files)} file(s) containing keyword '{keyword}' (case-insensitive match)\\n\")\n",
    "        return filtered_files\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\">> ❌ ERROR: Failed to retrieve file list. {e}\\n\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# STEP 5: DOWNLOAD FILES WITH RESUME AND STATUS BAR\n",
    "# -------------------------------------------------\n",
    "\n",
    "def download_selected_files(files_to_download, download_dir, session):\n",
    "    print(\"\\n========== STEP 5: INITIATING DOWNLOADS ==========\\n\")\n",
    "    print(\">> Subprocess: Downloading files with resume support and progress tracking\\n\")\n",
    "\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "    for i, file_info in enumerate(files_to_download):\n",
    "        file_id     = file_info['dataFile']['id']\n",
    "        filename    = file_info['dataFile']['filename']\n",
    "        total_size  = file_info['dataFile']['filesize']\n",
    "        full_path   = os.path.join(download_dir, filename)\n",
    "\n",
    "        print(f\"\\n[{i+1}/{len(files_to_download)}] Starting: {filename}\")\n",
    "\n",
    "        try:\n",
    "            download_url   = f\"https://dataverse.harvard.edu/api/access/datafile/{file_id}\"\n",
    "            headers        = {}\n",
    "            file_mode      = 'wb'\n",
    "            downloaded_size = 0\n",
    "\n",
    "            # Resume logic\n",
    "            if os.path.exists(full_path):\n",
    "                downloaded_size = os.path.getsize(full_path)\n",
    "                if downloaded_size < total_size:\n",
    "                    file_mode = 'ab'\n",
    "                    headers['Range'] = f'bytes={downloaded_size}-'\n",
    "                    print(f\">> Resuming from {downloaded_size / (1024*1024):.2f} MB (Partial download detected)\\n\")\n",
    "                elif downloaded_size == total_size:\n",
    "                    print(\">> File already complete. Skipping.\\n\")\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\">> Corrupt file detected (larger than expected). Restarting download.\\n\")\n",
    "                    downloaded_size = 0\n",
    "\n",
    "            # Begin download\n",
    "            with session.get(download_url, stream=True, headers=headers, timeout=90) as r:\n",
    "                r.raise_for_status()\n",
    "                with open(full_path, file_mode) as file:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        downloaded_size += len(chunk)\n",
    "                        file.write(chunk)\n",
    "                        percent = (downloaded_size / total_size) * 100 if total_size > 0 else 100\n",
    "                        status = (\n",
    "                            f\">> Downloaded: {downloaded_size / (1024*1024):.2f} / \"\n",
    "                            f\"{total_size / (1024*1024):.2f} MB ({percent:.1f}%)\"\n",
    "                        )\n",
    "                        sys.stdout.write(f\"\\r{status}\")\n",
    "                        sys.stdout.flush()\n",
    "\n",
    "            print(f\"\\n>> ✅ Completed: {filename}\")\n",
    "            print(\">> Reasoning: File size matched expected total. Download integrity confirmed.\\n\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\n>> ❌ Network error during download of {filename}: {e}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n>> ❌ Unexpected error with {filename}: {e}\\n\")\n",
    "\n",
    "    print(\"\\n========== ✅ ALL FILTERED DOWNLOADS COMPLETED ==========\\n\")\n",
    "\n",
    "\n",
    "# MAIN EXECUTION BLOCK\n",
    "# --------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n===================================================\")\n",
    "    print(\"        HARVARD DATAVERSE DOWNLOADER - STARTED     \")\n",
    "    print(\"===================================================\\n\")\n",
    "\n",
    "    session = configure_session()\n",
    "    files_to_download = get_filtered_files(dataset_doi, session, keyword=keyword_filter)\n",
    "\n",
    "    if files_to_download:\n",
    "        download_selected_files(files_to_download, download_directory, session)\n",
    "    else:\n",
    "        print(\">> No matching files found or error occurred. Exiting.\\n\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ff39677-e054-4b80-a1d7-b5aa15553dae",
   "metadata": {},
   "source": [
    "Output preview\n",
    "\n",
    "===================================================\n",
    "        HARVARD DATAVERSE DOWNLOADER - STARTED     \n",
    "===================================================\n",
    "\n",
    "\n",
    "========== STEP 3: CONFIGURING HTTP SESSION ==========\n",
    "\n",
    ">> Subprocess: Setting up retry strategy for network resilience\n",
    "\n",
    ">> ✅ Session configured to retry up to 5 times on transient errors.\n",
    "\n",
    "\n",
    "========== STEP 4: RETRIEVING AND FILTERING FILES ==========\n",
    "\n",
    ">> Subprocess: Accessing Dataverse API and filtering by keyword\n",
    "\n",
    ">> Requesting file list for DOI: doi:10.7910/DVN/V2C6G2\n",
    ">> API Endpoint: https://dataverse.harvard.edu/api/datasets/:persistentId/versions/:latest/files?persistentId=doi%3A10.7910%2FDVN%2FV2C6G2\n",
    "\n",
    ">> HTTP Status Code: 200\n",
    ">> Raw Response Preview:\n",
    "{\"status\":\"OK\",\"totalCount\":10523,\"data\":[{\"label\":\"ECMWF_S2S_BG_streamflow_20160101_01.nc\",\"restricted\":false,\"version\":1,\"datasetVersionId\":441100,\"dataFile\":{\"id\":10910117,\"persistentId\":\"\",\"filename\":\"ECMWF_S2S_BG_streamflow_20160101_01.nc\",\"contentType\":\"application/netcdf\",\"friendlyType\":\"Network Common Data Form\",\"filesize\":3349993,\"storageIdentifier\":\"s3://dvn-cloud:1954a24fcdd-0da28859bdcf\",\"rootDataFileId\":-1,\"md5\":\"a11767e5cdad30e82a4cdffc97e3eac6\",\"checksum\":{\"type\":\"MD5\",\"value\":\"a1\n",
    "\n",
    ">> ✅ Found 73 file(s) containing keyword 'ERA5L' (case-insensitive match)\n",
    "\n",
    "\n",
    "========== STEP 5: INITIATING DOWNLOADS ==========\n",
    "\n",
    ">> Subprocess: Downloading files with resume support and progress tracking\n",
    "\n",
    "\n",
    "[1/73] Starting: ERA5L_9km_BG_daily_streamflow_1951.nc\n",
    ">> File already complete. Downloaded.\n",
    "-----------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "[73/73] Starting: ERA5L_9km_BG_daily_streamflow_2023.nc\n",
    ">> File already complete. Downloaded.\n",
    "\n",
    "\n",
    "========== ✅ ALL FILTERED DOWNLOADS COMPLETED =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421b4e74-b1a5-421c-96cd-68e77d3c92a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c45a48e-00be-4e27-8418-75d01e5092f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc91582-c2cb-4347-bfb7-04d48c7552bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcc4b70-b8bb-454e-849d-2e28292813c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =================================================================================================\n",
    "#\n",
    "#   ** Harvard Dataverse Dataset Downloader (Bulk download) **\n",
    "#\n",
    "#   **Description:**\n",
    "#   This script downloads files from a specific Harvard Dataverse dataset. It automatically\n",
    "#   retrieves the complete file list and prompts the user to download either the first or\n",
    "#   second half of the files, allowing for large downloads to be split into two sessions.\n",
    "#\n",
    "#   **Key Features:**\n",
    "#   - **Direct Download:** No search or manual file selection required.\n",
    "#   - **Two-Step Downloading:** Automatically splits the full file list into two halves.\n",
    "#   - **Text-Based Progress:** Displays a clean, single-line download status for each file.\n",
    "#   - **Automatic Retries:** Automatically retries failed or timed-out connections.\n",
    "#   - **Resumable Downloads:** If a file download is interrupted, it can be resumed.\n",
    "#\n",
    "# =================================================================================================\n",
    "\n",
    "# 1. ===== Import Dependencies =====\n",
    "# ===================================\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "# =================================================================================================\n",
    "# 2. ===== User-Definable Options =====\n",
    "# ======================================\n",
    "# --- Instructions ---\n",
    "# Please modify the variables in this section to suit your needs.\n",
    "#\n",
    "# `dataset_doi`: The persistent identifier (DOI) for the target dataset.\n",
    "# `download_directory`: The full path to the folder where the files will be saved.\n",
    "#\n",
    "dataset_doi = \"doi:10.7910/DVN/V2C6G2\" # Corresponds to \"High-resolution gridded streamflow...\"\n",
    "download_directory = \"Observed_daily_dscharge/Raw_netcdf\"\n",
    "# =================================================================================================\n",
    "\n",
    "\n",
    "def get_files_and_choose_batch(persistent_id, session):\n",
    "    \"\"\"\n",
    "    Retrieves the full file list for a dataset and prompts the user to\n",
    "    select which half of the list to download.\n",
    "    \"\"\"\n",
    "    print(\"STEP 3: RETRIEVING AND PREPARING FILE LIST\")\n",
    "    print(\"---------------------------------------------\")\n",
    "    try:\n",
    "        files_api_url = f\"https://dataverse.harvard.edu/api/datasets/:persistentId/versions/:latest/files?persistentId={persistent_id}\"\n",
    "        print(f\"  > Requesting file list for DOI: {persistent_id}\")\n",
    "        meta_response = session.get(files_api_url, timeout=90)\n",
    "        meta_response.raise_for_status()\n",
    "        files_data = meta_response.json()['data']\n",
    "\n",
    "        if not files_data:\n",
    "            print(\"  > ❌ This dataset contains no files.\")\n",
    "            return []\n",
    "\n",
    "        print(f\"  ✅  Found a total of {len(files_data)} files in the dataset.\")\n",
    "\n",
    "        # Split the file list into two halves\n",
    "        midpoint = (len(files_data) + 1) // 2\n",
    "        first_half = files_data[:midpoint]\n",
    "        second_half = files_data[midpoint:]\n",
    "\n",
    "        print(f\"    - First Half contains: {len(first_half)} files.\")\n",
    "        print(f\"    - Second Half contains: {len(second_half)} files.\")\n",
    "\n",
    "        while True:\n",
    "            batch_choice = input(f\"\\n  > Which batch do you want to download? (Enter 1 or 2): \")\n",
    "            if batch_choice == '1':\n",
    "                print(f\"\\n  > Preparing to download the first half ({len(first_half)} files).\\n\")\n",
    "                return first_half\n",
    "            elif batch_choice == '2':\n",
    "                print(f\"\\n  > Preparing to download the second half ({len(second_half)} files).\\n\")\n",
    "                return second_half\n",
    "            else:\n",
    "                print(\"  > Invalid input. Please enter 1 or 2.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"\\n  > ❌ ERROR: Could not retrieve file list. {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def download_selected_files(files_to_download, download_dir, session):\n",
    "    \"\"\"\n",
    "    Downloads a list of selected files one by one.\n",
    "    \"\"\"\n",
    "    print(\"STEP 4: INITIATING DOWNLOADS\")\n",
    "    print(\"------------------------------\")\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    \n",
    "    for i, file_info in enumerate(files_to_download):\n",
    "        file_id = file_info['dataFile']['id']\n",
    "        filename = file_info['dataFile']['filename']\n",
    "        total_size = file_info['dataFile']['filesize']\n",
    "        full_path = os.path.join(download_dir, filename)\n",
    "\n",
    "        print(f\"\\n[{i+1}/{len(files_to_download)}] Starting download for: {filename}\")\n",
    "\n",
    "        try:\n",
    "            download_url = f\"https://dataverse.harvard.edu/api/access/datafile/{file_id}\"\n",
    "            headers = {}\n",
    "            file_mode = 'wb'\n",
    "            downloaded_size = 0\n",
    "\n",
    "            if os.path.exists(full_path):\n",
    "                downloaded_size = os.path.getsize(full_path)\n",
    "                if downloaded_size < total_size:\n",
    "                    file_mode = 'ab'\n",
    "                    headers['Range'] = f'bytes={downloaded_size}-'\n",
    "                    print(f\"  > Resuming from {downloaded_size / (1024*1024):.2f} MB...\")\n",
    "                elif downloaded_size == total_size:\n",
    "                    print(\"  > File already complete. Skipping.\")\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"  > Corrupt local file found. Restarting download.\")\n",
    "                    downloaded_size = 0\n",
    "\n",
    "            with session.get(download_url, stream=True, headers=headers, timeout=90) as r:\n",
    "                r.raise_for_status()\n",
    "                with open(full_path, file_mode) as file:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        downloaded_size += len(chunk)\n",
    "                        file.write(chunk)\n",
    "                        percent = (downloaded_size / total_size) * 100 if total_size > 0 else 100\n",
    "                        status = f\"> Downloaded {downloaded_size / (1024*1024):.2f} / {total_size / (1024*1024):.2f} MB ({percent:.1f}%)\"\n",
    "                        sys.stdout.write(f\"\\r{status}\")\n",
    "                        sys.stdout.flush()\n",
    "            print(f\"\\n  ✅  Download complete: {filename}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\n  > ❌ DOWNLOAD FAILED for {filename}. {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n  > ❌ An UNEXPECTED ERROR occurred with {filename}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n=============================================\")\n",
    "    print(\"   HARVARD DATAVERSE DOWNLOADER - STARTED\")\n",
    "    print(\"=============================================\\n\")\n",
    "\n",
    "    print(\"STEP 2: CONFIGURING RESILIENT HTTP SESSION\")\n",
    "    print(\"--------------------------------------------\")\n",
    "    session = requests.Session()\n",
    "    retry_strategy = Retry(total=5, status_forcelist=[429, 500, 502, 503, 504], backoff_factor=1)\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    print(\"  > Session configured to retry up to 5 times on connection errors.\\n\")\n",
    "    \n",
    "    # Retrieve the file list and prompt the user to choose a batch\n",
    "    files_to_download = get_files_and_choose_batch(dataset_doi, session)\n",
    "\n",
    "    # Proceed with download if a batch was selected\n",
    "    if files_to_download:\n",
    "        download_selected_files(files_to_download, download_directory, session)\n",
    "        print(\"\\n\\nAll selected downloads have been processed.\")\n",
    "    else:\n",
    "        print(\"\\nNo files were selected for download or an error occurred. Exiting script.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7e9b9d5-c0c0-4e6e-8cdc-6d175d75a4e8",
   "metadata": {},
   "source": [
    "Output preview\n",
    "\n",
    "=============================================\n",
    "   HARVARD DATAVERSE DOWNLOADER - STARTED\n",
    "=============================================\n",
    "\n",
    "STEP 2: CONFIGURING RESILIENT HTTP SESSION\n",
    "--------------------------------------------\n",
    "  > Session configured to retry up to 5 times on connection errors.\n",
    "\n",
    "STEP 3: RETRIEVING AND PREPARING FILE LIST\n",
    "---------------------------------------------\n",
    "  > Requesting file list for DOI: doi:10.7910/DVN/V2C6G2\n",
    "  ✅  Found a total of 10523 files in the dataset.\n",
    "    - First Half contains: 5262 files.\n",
    "    - Second Half contains: 5261 files.\n",
    "  > Which batch do you want to download? (Enter 1 or 2):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
