{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892be0c-730f-4266-842c-f5aeed87fdb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════════════\n",
    "# 📦 DEPENDENCIES\n",
    "# ════════════════════════════════════════════════════════════════════════\n",
    "# Purpose: Import all necessary libraries for data handling, plotting, statistics, and console styling.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore, Style, init\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from hydroeval import nse, pbias, kge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Initialize colorama for colored console output\n",
    "init(autoreset=True)\n",
    "\n",
    "\n",
    "# ════════════════════════════════════════════════════════════════════════\n",
    "# ⚙️ USER CONFIGURATION\n",
    "# ════════════════════════════════════════════════════════════════════════\n",
    "# Purpose: This section contains all editable options. Modify them to match your project.\n",
    "\n",
    "class Configuration:\n",
    "    \"\"\"\n",
    "    Holds all user-configurable settings for the analysis.\n",
    "    \"\"\"\n",
    "    # --- 1. Output Control ---\n",
    "    # Description: Choose whether to save plots as PDF files.\n",
    "    # - Set to True to save a PDF for each subbasin.\n",
    "    # - Set to False to run the script without creating any files.\n",
    "    SAVE_PLOTS_TO_PDF = True\n",
    "\n",
    "    # --- 2. Input/Output Data Paths ---\n",
    "    # Description: Define the locations for your data files.\n",
    "    BASE_DIR = Path(\"Your_project_folder\")\n",
    "    COMPARISON_DIR = BASE_DIR / \"Discharge_comparison\" # Reads merged CSVs from here\n",
    "    PDF_OUTPUT_DIR = BASE_DIR / \"PDF_Plot_Reports\"     # Saves PDF reports here\n",
    "\n",
    "    # --- 3. Time Periods for Analysis ---\n",
    "    # Description: Specify the date ranges for calibration and validation.\n",
    "    CALIBRATION_START = \"2000-01-01\"\n",
    "    CALIBRATION_END = \"2009-12-31\"\n",
    "    VALIDATION_START = \"2010-01-01\"\n",
    "    VALIDATION_END = \"2015-12-31\"\n",
    "\n",
    "    # --- 4. Subbasin Selection ---\n",
    "    # Description: Choose which subbasins to process.\n",
    "    # - To run ALL subbasins, leave the list empty: []\n",
    "    # - To run specific subbasins, add their names: [\"Subbasin_1\", \"Subbasin_42\"]\n",
    "    SUBBASINS_TO_RUN = []\n",
    "\n",
    "    # --- 5. Column Names in Source Files ---\n",
    "    # Description: Ensure these names match the column headers in your CSV files.\n",
    "    DATE_COL = \"Year_Month\"\n",
    "    DATE_FORMAT = \"%Y_%b\"\n",
    "    OBS_COL = \"OBSERVED\"\n",
    "    SIM_COL = \"SIMULATED\"\n",
    "\n",
    "    # --- 6. Seasonal Definitions ---\n",
    "    # Description: Define the months for each season.\n",
    "    SEASONS = {\n",
    "        'Wet Season': [6, 7, 8, 9, 10],         # June - October\n",
    "        'Dry Season': [11, 12, 1, 2, 3, 4, 5]    # November - May\n",
    "    }\n",
    "\n",
    "    # --- 7. Plot Style Customization ---\n",
    "    # Description: Define the visual appearance of the plots.\n",
    "    PLOT_STYLES = {\n",
    "        'observed_color': 'black',\n",
    "        'simulated_color': 'crimson',\n",
    "        'calibration_fill': 'mediumseagreen',\n",
    "        'validation_fill': 'cornflowerblue',\n",
    "        'fill_opacity': 0.25,\n",
    "        'line_width': 1.5,\n",
    "        'scatter_marker_size': 10,\n",
    "        'one_to_one_line_color': 'navy'\n",
    "    }\n",
    "\n",
    "    # --- 8. Statistics Configuration ---\n",
    "    # Description: Parameters for metric calculations.\n",
    "    STD_TYPE = 1               # 1 = sample std (ddof=1), 0 = population std (ddof=0)\n",
    "    UNCERTAINTY_PERCENT = 0.5  # Fractional uncertainty for P-factor & R-factor\n",
    "\n",
    "    # Description: Choose which metrics to display in the summary table.\n",
    "    STATISTICS_TO_DISPLAY = [\n",
    "        \"NSE\", \"KGE (2012)\", \"R²\", \"Pearson r\", \"RSR\", \"PBIAS (%)\", \"RMSE\",\n",
    "        \"MAE\", \"MAPE (%)\", \"Bias\", \"SDR\", \"P-factor\", \"R-factor\"\n",
    "    ]\n",
    "\n",
    "\n",
    "# Instantiate the configuration object\n",
    "cfg = Configuration()\n",
    "\n",
    "\n",
    "# ════════════════════════════════════════════════════════════════════════\n",
    "# 🛠️ HELPER & STATISTICAL FUNCTIONS\n",
    "# ════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def safe_divide(num: float, den: float, default: float = np.nan) -> float:\n",
    "    \"\"\"Performs division, returning a default value if the denominator is zero or invalid.\"\"\"\n",
    "    return num / den if den and np.isfinite(den) and den != 0 else default\n",
    "\n",
    "def compute_metrics(obs: np.ndarray, sim: np.ndarray) -> dict:\n",
    "    \"\"\"Calculates a comprehensive set of performance metrics.\"\"\"\n",
    "    obs, sim = np.asarray(obs).ravel(), np.asarray(sim).ravel()\n",
    "    mask = ~np.isnan(obs) & ~np.isnan(sim)\n",
    "    obs, sim = obs[mask], sim[mask]\n",
    "\n",
    "    if obs.size < 2: return {metric: np.nan for metric in cfg.STATISTICS_TO_DISPLAY}\n",
    "\n",
    "    def get_value(result):\n",
    "        \"\"\"Safely extracts the primary value from a metric function's result.\"\"\"\n",
    "        if isinstance(result, (list, np.ndarray)):\n",
    "            return result[0]\n",
    "        return result\n",
    "\n",
    "    mean_obs, std_obs = obs.mean(), obs.std(ddof=cfg.STD_TYPE)\n",
    "    mean_sim, std_sim = sim.mean(), sim.std(ddof=cfg.STD_TYPE)\n",
    "    diff = sim - obs\n",
    "    rmse_val = np.sqrt(np.mean(diff**2))\n",
    "    pearson_r = np.corrcoef(obs, sim)[0, 1]\n",
    "    \n",
    "    delta = np.abs(sim) * cfg.UNCERTAINTY_PERCENT\n",
    "    p_factor = np.sum((obs >= (sim - delta)) & (obs <= (sim + delta))) / len(obs)\n",
    "    r_factor = safe_divide(np.mean(2 * delta), std_obs)\n",
    "    mape = np.nanmean(np.abs(diff / np.where(obs == 0, np.nan, obs))) * 100\n",
    "\n",
    "    return {\n",
    "        \"NSE\": get_value(nse(sim, obs)),\n",
    "        \"KGE (2012)\": get_value(kge(sim, obs)),\n",
    "        \"R²\": r2_score(obs, sim),\n",
    "        \"Pearson r\": pearson_r,\n",
    "        \"RSR\": safe_divide(rmse_val, std_obs),\n",
    "        \"PBIAS (%)\": pbias(sim, obs),\n",
    "        \"RMSE\": rmse_val,\n",
    "        \"MAE\": np.mean(np.abs(diff)),\n",
    "        \"MAPE (%)\": mape,\n",
    "        \"Bias\": diff.mean(),\n",
    "        \"SDR\": safe_divide(std_sim, std_obs),\n",
    "        \"P-factor\": p_factor,\n",
    "        \"R-factor\": r_factor\n",
    "    }\n",
    "\n",
    "def process_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Prepares the dataframe for analysis by adding date components and season columns.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "    df_processed['Year'] = df_processed['Date'].dt.year\n",
    "    df_processed['Month'] = df_processed['Date'].dt.month\n",
    "    season_map = {m: s for s, months in cfg.SEASONS.items() for m in months}\n",
    "    df_processed['Season'] = df_processed['Month'].map(season_map)\n",
    "    df_processed = df_processed.dropna(subset=[cfg.OBS_COL, cfg.SIM_COL])\n",
    "    return df_processed\n",
    "\n",
    "\n",
    "# ════════════════════════════════════════════════════════════════════════\n",
    "# 📊 PLOTTING FUNCTIONS\n",
    "# ════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def plot_statistics_table(stats: dict, title: str) -> plt.Figure:\n",
    "    \"\"\"Creates a clean table of statistical metrics as a plot figure.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.set_title(title, fontsize=16, weight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(stats, orient='index').round(5)\n",
    "    df.columns = [\"Calibration\", \"Validation\", \"Full Period\"]\n",
    "    \n",
    "    table = ax.table(cellText=df.values, colLabels=df.columns, rowLabels=df.index,\n",
    "                     loc='center', cellLoc='center')\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 1.8)\n",
    "    \n",
    "    for (row, col), cell in table.get_celld().items():\n",
    "        if row == 0 or col == -1: cell.set_text_props(weight='bold')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_combined_hydrograph(df: pd.DataFrame, title: str) -> plt.Figure:\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    ax.plot(df['Date'], df[cfg.OBS_COL], label='Observed', color=cfg.PLOT_STYLES['observed_color'], lw=cfg.PLOT_STYLES['line_width'])\n",
    "    ax.plot(df['Date'], df[cfg.SIM_COL], label='Simulated', color=cfg.PLOT_STYLES['simulated_color'], lw=cfg.PLOT_STYLES['line_width'], alpha=0.9)\n",
    "    ax.axvspan(pd.to_datetime(cfg.CALIBRATION_START), pd.to_datetime(cfg.CALIBRATION_END),\n",
    "               color=cfg.PLOT_STYLES['calibration_fill'], alpha=cfg.PLOT_STYLES['fill_opacity'], label='Calibration')\n",
    "    ax.axvspan(pd.to_datetime(cfg.VALIDATION_START), pd.to_datetime(cfg.VALIDATION_END),\n",
    "               color=cfg.PLOT_STYLES['validation_fill'], alpha=cfg.PLOT_STYLES['fill_opacity'], label='Validation')\n",
    "    ax.set_title(title, fontsize=16, weight='bold')\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Flow ($m^3/s$)\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_scatter(df: pd.DataFrame, title: str) -> plt.Figure:\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    max_val = max(df[cfg.OBS_COL].max(), df[cfg.SIM_COL].max()) * 1.05\n",
    "    ax.scatter(df[cfg.OBS_COL], df[cfg.SIM_COL], color=cfg.PLOT_STYLES['simulated_color'],\n",
    "               s=cfg.PLOT_STYLES['scatter_marker_size'], alpha=0.7)\n",
    "    ax.plot([0, max_val], [0, max_val], color=cfg.PLOT_STYLES['one_to_one_line_color'],\n",
    "            linestyle='--', label='1:1 Line')\n",
    "    r2 = r2_score(df[cfg.OBS_COL], df[cfg.SIM_COL])\n",
    "    ax.text(0.05, 0.95, f'$R^2 = {r2:.3f}$', transform=ax.transAxes, fontsize=12,\n",
    "            verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', fc='wheat', alpha=0.5))\n",
    "    ax.set_title(title, weight='bold')\n",
    "    ax.set_xlabel(\"Observed Flow ($m^3/s$)\")\n",
    "    ax.set_ylabel(\"Simulated Flow ($m^3/s$)\")\n",
    "    ax.set_xlim(0, max_val)\n",
    "    ax.set_ylim(0, max_val)\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_flow_duration_curve(df: pd.DataFrame, title: str) -> plt.Figure:\n",
    "    obs_sorted = np.sort(df[cfg.OBS_COL])[::-1]\n",
    "    sim_sorted = np.sort(df[cfg.SIM_COL])[::-1]\n",
    "    exceedance = np.arange(1, len(obs_sorted) + 1) / len(obs_sorted) * 100\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.plot(exceedance, obs_sorted, label='Observed', color=cfg.PLOT_STYLES['observed_color'])\n",
    "    ax.plot(exceedance, sim_sorted, label='Simulated', color=cfg.PLOT_STYLES['simulated_color'])\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_title(title, weight='bold')\n",
    "    ax.set_xlabel(\"Exceedance Probability (%)\")\n",
    "    ax.set_ylabel(\"Flow ($m^3/s$)\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, which=\"both\", linestyle='--', alpha=0.6)\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_seasonal_barchart(df: pd.DataFrame, title: str) -> plt.Figure:\n",
    "    seasonal_stats = df.groupby('Season')[[cfg.OBS_COL, cfg.SIM_COL]].mean()\n",
    "    season_order = list(cfg.SEASONS.keys())\n",
    "    seasonal_stats = seasonal_stats.reindex(season_order)\n",
    "    x = np.arange(len(season_order))\n",
    "    bar_width = 0.35\n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "    ax.bar(x - bar_width / 2, seasonal_stats[cfg.OBS_COL], bar_width, label='Observed', color=cfg.PLOT_STYLES['observed_color'])\n",
    "    ax.bar(x + bar_width / 2, seasonal_stats[cfg.SIM_COL], bar_width, label='Simulated', color=cfg.PLOT_STYLES['simulated_color'])\n",
    "    ax.set_xticks(x, season_order)\n",
    "    ax.set_ylabel(\"Average Flow ($m^3/s$)\")\n",
    "    ax.set_title(title, weight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ════════════════════════════════════════════════════════════════════════\n",
    "# 🔁 MAIN SCRIPT EXECUTION\n",
    "# ════════════════════════════════════════════════════════════════════════\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the entire workflow.\"\"\"\n",
    "    print(Fore.CYAN + Style.BRIGHT + \"--- SCRIPT INITIALIZATION ---\")\n",
    "    if cfg.SAVE_PLOTS_TO_PDF:\n",
    "        print(f\"- Output Mode: PDF reports will be saved to: {cfg.PDF_OUTPUT_DIR}\")\n",
    "        cfg.PDF_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "    else:\n",
    "        print(\"- Output Mode: No files will be saved.\")\n",
    "    print(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "    # --- Step 1: Discover, Validate, and Sort Input Files ---\n",
    "    print(Fore.CYAN + Style.BRIGHT + \"--- STEP 1: DISCOVERING AND SORTING INPUT FILES ---\\n\")\n",
    "    input_files = list(cfg.COMPARISON_DIR.glob(\"*.csv\"))\n",
    "    if cfg.SUBBASINS_TO_RUN:\n",
    "        input_files = [f for f in input_files if f.stem in cfg.SUBBASINS_TO_RUN]\n",
    "    if not input_files:\n",
    "        print(Fore.RED + f\"CRITICAL ERROR: No files found to process. Exiting.\")\n",
    "        return\n",
    "    def sort_key(p): return (int(p.stem.split('_')[-1]) if p.stem.split('_')[-1].isdigit() else p.stem)\n",
    "    input_files = sorted(input_files, key=sort_key)\n",
    "    print(f\"-> Found and sorted {len(input_files)} subbasin files for processing.\\n\")\n",
    "\n",
    "    # --- Step 2: Process Each Subbasin Sequentially ---\n",
    "    print(Fore.CYAN + Style.BRIGHT + \"--- STEP 2: GENERATING REPORTS SEQUENTIALLY ---\\n\")\n",
    "    \n",
    "    success_count = 0\n",
    "    for file_path in tqdm(input_files, desc=\"Generating Reports\"):\n",
    "        subbasin_name = file_path.stem\n",
    "        try:\n",
    "            df_full = pd.read_csv(file_path)\n",
    "            \n",
    "            required_cols = {cfg.DATE_COL, cfg.OBS_COL, cfg.SIM_COL}\n",
    "            if not required_cols.issubset(df_full.columns):\n",
    "                tqdm.write(Fore.YELLOW + f\"WARNING: Skipping {subbasin_name}. Missing required columns. Found: {list(df_full.columns)}\")\n",
    "                continue\n",
    "\n",
    "            df_full['Date'] = pd.to_datetime(df_full[cfg.DATE_COL], format=cfg.DATE_FORMAT)\n",
    "            min_year = pd.to_datetime(cfg.CALIBRATION_START).year\n",
    "            max_year = pd.to_datetime(cfg.VALIDATION_END).year\n",
    "            df_filtered = df_full[df_full['Date'].dt.year.between(min_year, max_year)]\n",
    "            df_processed = process_dataframe(df_filtered)\n",
    "\n",
    "            if df_processed.empty:\n",
    "                tqdm.write(Fore.YELLOW + f\"WARNING: Skipping {subbasin_name} (no data in {min_year}-{max_year} range).\")\n",
    "                continue\n",
    "\n",
    "            df_cal = df_processed[df_processed['Date'].between(cfg.CALIBRATION_START, cfg.CALIBRATION_END)]\n",
    "            df_val = df_processed[df_processed['Date'].between(cfg.VALIDATION_START, cfg.VALIDATION_END)]\n",
    "            stats_cal = compute_metrics(df_cal[cfg.OBS_COL].values, df_cal[cfg.SIM_COL].values)\n",
    "            stats_val = compute_metrics(df_val[cfg.OBS_COL].values, df_val[cfg.SIM_COL].values)\n",
    "            stats_full = compute_metrics(df_processed[cfg.OBS_COL].values, df_processed[cfg.SIM_COL].values)\n",
    "            combined_stats = {metric: [stats_cal.get(metric, np.nan), stats_val.get(metric, np.nan), stats_full.get(metric, np.nan)]\n",
    "                              for metric in cfg.STATISTICS_TO_DISPLAY}\n",
    "\n",
    "            if cfg.SAVE_PLOTS_TO_PDF:\n",
    "                pdf_path = cfg.PDF_OUTPUT_DIR / f\"{subbasin_name}_Performance_Report.pdf\"\n",
    "                with PdfPages(pdf_path) as pdf:\n",
    "                    pdf.savefig(plot_statistics_table(combined_stats, f\"Performance Metrics: {subbasin_name}\"))\n",
    "                    pdf.savefig(plot_combined_hydrograph(df_processed, f\"Hydrograph: {subbasin_name}\"))\n",
    "                    pdf.savefig(plot_scatter(df_processed, f\"Observed vs. Simulated (Full Period): {subbasin_name}\"))\n",
    "                    pdf.savefig(plot_flow_duration_curve(df_processed, f\"Flow Duration Curve (Full Period): {subbasin_name}\"))\n",
    "                    pdf.savefig(plot_seasonal_barchart(df_processed, f\"Seasonal Average Flow (Full Period): {subbasin_name}\"))\n",
    "\n",
    "            plt.close('all')\n",
    "            success_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            tqdm.write(Fore.RED + f\"\\nERROR processing {subbasin_name}: {e}\")\n",
    "            plt.close('all')\n",
    "\n",
    "    # --- Final Notification ---\n",
    "    print(\"\\n\" + Fore.CYAN + Style.BRIGHT + \"--- PROCESSING COMPLETE ---\")\n",
    "    if cfg.SAVE_PLOTS_TO_PDF:\n",
    "        print(Fore.GREEN + f\"Successfully generated reports for {success_count}/{len(input_files)} subbasins in: {cfg.PDF_OUTPUT_DIR}\")\n",
    "    else:\n",
    "        print(Fore.GREEN + f\"Script finished. Processed {success_count}/{len(input_files)} subbasins.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164cfaa8-01f9-4139-bba0-c9af9dc7bd51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "37f1f05a-7fc9-4de4-b106-5931dc5aa0d9",
   "metadata": {},
   "source": [
    "Output Preview\n",
    "\n",
    "--- SCRIPT INITIALIZATION ---\n",
    "- Output Mode: PDF reports will be saved to: PDF_Plot_Reports\n",
    "--------------------------------------------------\n",
    "\n",
    "--- STEP 1: DISCOVERING AND SORTING INPUT FILES ---\n",
    "\n",
    "-> Found and sorted 415 subbasin files for processing.\n",
    "\n",
    "--- STEP 2: GENERATING REPORTS SEQUENTIALLY ---\n",
    "\n",
    "Generating Reports: 100%|████████████████████████████████████████████████████████████| 415/415 [13:38<00:00,  1.97s/it]\n",
    "\n",
    "\n",
    "--- PROCESSING COMPLETE ---\n",
    "Successfully generated reports for 415/415 subbasins in: PDF_Plot_Reports\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
