{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05140f78-af2b-437d-a3b6-f6dfef153633",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ================================================================================================\n",
    "# \tPYTHON SCRIPT: HIGH-PERFORMANCE NETCDF TIME-SERIES EXTRACTOR (PARALLEL & RESILIENT)\n",
    "# ================================================================================================\n",
    "#\n",
    "# \t❖ PURPOSE:\n",
    "# \t \tA robust, **parallel**, and memory-efficient workflow to process multi-year NetCDF data\n",
    "# \t \t(e.g., daily streamflow) for specific geographical points (subbasins/outlets).\n",
    "#\n",
    "# \t \t1. Extracts time-series data from multiple yearly NetCDF files **in parallel** (Phase 1).\n",
    "# \t \t2. Merges the yearly data for each point into a single, complete time-series file (Phase 2).\n",
    "# \t \t3. Uses intelligent resume logic with **verification checks** to provide continuous status feedback.\n",
    "# \t \t4. **NEW:** Provides a real-time, formatted download/verification report in the console.\n",
    "#\n",
    "# \t❖ REQUIREMENTS:\n",
    "# \t \t- xarray\n",
    "# \t \t- pandas\n",
    "# \t \t- tqdm (for status bars)\n",
    "# \t \t- numpy\n",
    "# \t \t- multiprocessing (built-in)\n",
    "#\n",
    "# ================================================================================================\n",
    "\n",
    "# STEP 1: IMPORT MODULES\n",
    "# ----------------------\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "from typing import Dict, Any, List, Tuple\n",
    "from tqdm import tqdm # Used for clear status bars\n",
    "import multiprocessing as mp\n",
    "import sys \n",
    "\n",
    "# Global dictionary for process-local caching (one cache per worker process)\n",
    "# This prevents redundant file opening within the same worker.\n",
    "worker_datasets: Dict[str, xr.Dataset] = {}\n",
    "\n",
    "# Console report template constants\n",
    "REPORT_HEADER = \"| Point ID & File Stem \t \t \t \t \t \t | Status\t | Action\t \"\n",
    "REPORT_SEPARATOR = \"-\" * 75\n",
    "\n",
    "# ================================================================================================\n",
    "# \tSTEP 2: USER-EDITABLE SETTINGS (MAXIMUM FLEXIBILITY)\n",
    "# ================================================================================================\n",
    "\n",
    "# --- A. FILE PATHS & DIRECTORIES ---\n",
    "\n",
    "# 1. INPUT DIRECTORY (The folder containing all your NetCDF files)\n",
    "NC_INPUT_DIRECTORY: str = \"Observed_daily_dscharge/Raw_netcdf\"\n",
    "\n",
    "# 2. POINT FILE PATH (The CSV containing the longitudes and latitudes of the outlets/subbasins)\n",
    "POINT_CSV_FILE_PATH: str = \"Outlet_subbasin.csv\"\n",
    "\n",
    "# --- DERIVED OUTPUT PATHS (DO NOT EDIT THESE) ---\n",
    "INPUT_ROOT_DIR = pathlib.Path(NC_INPUT_DIRECTORY)\n",
    "# Temporary directory for yearly extracted CSVs (will be created/deleted)\n",
    "YEARLY_DATA_DIR: str = str(INPUT_ROOT_DIR / \"01_Intermediate_Yearly_CSVs\")\n",
    "# Final directory for the single, merged time-series CSV for each point\n",
    "FINAL_MERGED_OUTPUT_DIR: str = str(INPUT_ROOT_DIR / \"02_Final_Merged_Streamflow\")\n",
    "\n",
    "# --- B. NETCDF FILE SELECTION ---\n",
    "# Pattern to match NetCDF files (e.g., \"*.nc\" for all files, or \"ERA5L_*.nc\")\n",
    "NC_FILE_PATTERN: str = \"*.nc\"\n",
    "# Process a subset of files: If empty list [], all files matching the pattern will be processed.\n",
    "NC_FILES_TO_PROCESS: List[str] = [\"ERA5L_9km_BG_daily_streamflow_1952.nc\"]\n",
    "\n",
    "# --- C. INPUT DATA CONFIGURATION ---\n",
    "POINT_ID_FIELD: str = \"NAME\"\n",
    "LONGITUDE_FIELD: str = \"LONG\"\n",
    "LATITUDE_FIELD: str = \"LAT\"\n",
    "NETCDF_VARIABLE: str = \"flw\"\n",
    "\n",
    "# --- D. OUTPUT DATA CONFIGURATION ---\n",
    "OUTPUT_COLUMN_NAMES: Dict[str, str] = {\"time\": \"DATE\", \"value\": \"DISCHARGE\"}\n",
    "\n",
    "# --- E. PARALLELISM, RESILIENCE & CLEANUP ---\n",
    "# Maximum number of CPU cores to use for the heavy extraction phase.\n",
    "CPU_CORES_TO_USE: int = 6 \n",
    "# If True, files in FINAL_MERGED_OUTPUT_DIR will be overwritten.\n",
    "OVERWRITE_EXISTING: bool = False\n",
    "# If True, the temporary YEARLY_DATA_DIR is deleted after a successful run.\n",
    "CLEANUP_YEARLY_DIR: bool = False\n",
    "\n",
    "# ================================================================================================\n",
    "# \tSTEP 3: HELPER FUNCTIONS (NETCDF CACHING AND REPORTING)\n",
    "# ================================================================================================\n",
    "\n",
    "def open_nc_dataset_cached(nc_file_path: str) -> xr.Dataset:\n",
    "\t\"\"\"\n",
    "\tOpens a NetCDF dataset using a process-local cache.\n",
    "\tThis prevents redundant opening of the same file within a single worker process.\n",
    "\tNote: We rely on the process terminating to release the file handle, which is \n",
    "\tstandard practice for multiprocessing pools.\n",
    "\t\"\"\"\n",
    "\tglobal worker_datasets\n",
    "\tif nc_file_path not in worker_datasets:\n",
    "\t\t# Crucial for multiprocessing stability: use cache=False to prevent Dask/xarray issues\n",
    "\t\tworker_datasets[nc_file_path] = xr.open_dataset(nc_file_path, engine='netcdf4', cache=False)\n",
    "\treturn worker_datasets[nc_file_path]\n",
    "\n",
    "def report_task_result(point_id: str, file_stem: str, status: str, action: str):\n",
    "\t\"\"\"Prints a single line to the console using the formatted report template.\"\"\"\n",
    "\t\n",
    "\t# Truncate point_id + file_stem to fit the first column (40 chars max)\n",
    "\tname = f\"{point_id} | {file_stem}\"\n",
    "\tif len(name) > 40:\n",
    "\t\tname = name[:37] + \"...\"\n",
    "\n",
    "\t# Ensure output is printed correctly with status bar in mind\n",
    "\ttqdm.write(\n",
    "\t\tf\"| {name:<40} | {status:<7} | {action:<10}\"\n",
    "\t)\n",
    "\n",
    "# ================================================================================================\n",
    "# \tSTEP 4: CORE WORKER FUNCTIONS (PARALLEL & SEQUENTIAL LOGIC)\n",
    "# ================================================================================================\n",
    "\n",
    "def extraction_worker_parallel(task_data: Tuple[pd.Series, str]) -> Tuple[bool, str, str, str]:\n",
    "\t\"\"\"\n",
    "\tWorker function executed by the Process Pool for Phase 1.\n",
    "\tReturns: (success_status, point_id, nc_stem, message)\n",
    "\t\"\"\"\n",
    "\tpoint_data, nc_file_path = task_data\n",
    "\tnc_stem = pathlib.Path(nc_file_path).stem # Initialize stem for error reporting\n",
    "\n",
    "\ttry:\n",
    "\t\t# --- 1. SETUP ---\n",
    "\t\tpoint_id = str(point_data[POINT_ID_FIELD])\n",
    "\t\tyearly_dir = pathlib.Path(YEARLY_DATA_DIR) / nc_stem\n",
    "\t\tyearly_dir.mkdir(parents=True, exist_ok=True)\n",
    "\t\tyearly_output_path = yearly_dir / f\"{point_id}.csv\"\n",
    "\n",
    "\t\t# --- 2. VERIFIED RESUME/SKIP CHECK ---\n",
    "\t\tif not OVERWRITE_EXISTING and yearly_output_path.exists():\n",
    "\t\t\tds = open_nc_dataset_cached(nc_file_path)\n",
    "\t\t\texpected_records = len(ds['time'])\n",
    "\t\t\t\n",
    "\t\t\t# Count lines efficiently\n",
    "\t\t\twith open(yearly_output_path, 'rb') as f:\n",
    "\t\t\t\tactual_lines = sum(1 for line in f)\n",
    "\t\t\t\n",
    "\t\t\t# Check if CSV line count (data rows + 1 header) matches expected records\n",
    "\t\t\tif actual_lines == expected_records + 1:\n",
    "\t\t\t\treturn True, point_id, nc_stem, \"SKIPPED (VERIFIED)\"\n",
    "\t\t\telse:\n",
    "\t\t\t\t# File is corrupt/incomplete. Re-process.\n",
    "\t\t\t\tos.remove(yearly_output_path)\n",
    "\t\t\t\t# Code proceeds to extraction (3)\n",
    "\n",
    "\t\t# --- 3. EXTRACTION (If not skipped) ---\n",
    "\t\tlon = point_data[LONGITUDE_FIELD]\n",
    "\t\tlat = point_data[LATITUDE_FIELD]\n",
    "\t\tds = open_nc_dataset_cached(nc_file_path)\n",
    "\n",
    "\t\t# Cleaner spatial selection using dictionary for sel\n",
    "\t\tdata_slice = ds[NETCDF_VARIABLE].sel(\n",
    "\t\t\t{'lon': lon, 'lat': lat},\n",
    "\t\t\tmethod='nearest'\n",
    "\t\t)\n",
    "\n",
    "\t\t# 4. Convert to Pandas DataFrame and save\n",
    "\t\t# to_dataframe names the column automatically after the variable\n",
    "\t\tdf = data_slice.to_dataframe().reset_index() \n",
    "\t\tdf = df.rename(columns={'time': 'time', NETCDF_VARIABLE: 'value'})\n",
    "\t\tdf[['time', 'value']].to_csv(yearly_output_path, index=False)\n",
    "\n",
    "\t\treturn True, point_id, nc_stem, \"PROCESSED\"\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\t# Return the error message and the file/point context\n",
    "\t\terror_msg = f\"Error: {e}\"\n",
    "\t\treturn False, str(point_data.get(POINT_ID_FIELD, 'UNKNOWN')), nc_stem, error_msg\n",
    "\n",
    "\n",
    "def merge_worker_sequential(point_id: str, nc_file_stems: List[str]) -> Tuple[bool, str, str]:\n",
    "\t\"\"\"\n",
    "\tSequential worker function for Phase 2: Merges all yearly CSVs for one point into a single file.\n",
    "\t\"\"\"\n",
    "\tfinal_output_path = pathlib.Path(FINAL_MERGED_OUTPUT_DIR) / f\"{point_id}.csv\"\n",
    "\n",
    "\t# Smart resume check for final file\n",
    "\tif not OVERWRITE_EXISTING and final_output_path.exists():\n",
    "\t\ttry:\n",
    "\t\t\texpected_file_count = len(nc_file_stems)\n",
    "\t\t\t# Only count intermediate files that actually exist\n",
    "\t\t\tactual_yearly_files = sum(1 for stem in nc_file_stems if (pathlib.Path(YEARLY_DATA_DIR) / stem / f\"{point_id}.csv\").exists())\n",
    "\n",
    "\t\t\tif actual_yearly_files == expected_file_count and final_output_path.stat().st_size > 1024:\n",
    "\t\t\t\treturn True, point_id, f\"Skipped (Already fully merged: {final_output_path.name})\"\n",
    "\t\texcept Exception:\n",
    "\t\t\t# Fall through and re-process if any check fails\n",
    "\t\t\tpass \n",
    "\n",
    "\ttry:\n",
    "\t\tyearly_dfs = []\n",
    "\t\tfor stem in nc_file_stems:\n",
    "\t\t\tyearly_path = pathlib.Path(YEARLY_DATA_DIR) / stem / f\"{point_id}.csv\"\n",
    "\t\t\tif yearly_path.exists():\n",
    "\t\t\t\tdf = pd.read_csv(\n",
    "\t\t\t\t\tyearly_path,\n",
    "\t\t\t\t\tparse_dates=['time'],\n",
    "\t\t\t\t\tdtype={'value': np.float64}\n",
    "\t\t\t\t)\n",
    "\t\t\t\tyearly_dfs.append(df)\n",
    "\n",
    "\t\tif not yearly_dfs:\n",
    "\t\t\treturn False, point_id, f\"Error: No yearly data found for Point ID {point_id}.\"\n",
    "\n",
    "\t\tmerged_df = pd.concat(yearly_dfs, ignore_index=True)\n",
    "\t\t# Ensure correct chronological order and handle potential duplicates (e.g., from incomplete runs)\n",
    "\t\tmerged_df = merged_df.sort_values(by='time').drop_duplicates(subset='time', keep='last')\n",
    "\n",
    "\t\tmerged_df.rename(columns=OUTPUT_COLUMN_NAMES, inplace=True)\n",
    "\n",
    "\t\tfinal_output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\t\tmerged_df.to_csv(final_output_path, index=False)\n",
    "\n",
    "\t\treturn True, point_id, str(final_output_path)\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\treturn False, point_id, f\"Error merging data: {e}\"\n",
    "\n",
    "# ================================================================================================\n",
    "# \tSTEP 5: MAIN WORKFLOW PHASES (PARALLEL EXECUTION)\n",
    "# ================================================================================================\n",
    "\n",
    "def prepare_data_and_files(points_df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "\t\"\"\"Loads points, finds NC files, and prepares the processing list.\"\"\"\n",
    "\t\n",
    "\t# 1. Load Points\n",
    "\tpoints_df[POINT_ID_FIELD] = points_df[POINT_ID_FIELD].astype(str)\n",
    "\tprint(f\"Loaded {len(points_df)} points from {pathlib.Path(POINT_CSV_FILE_PATH).name}.\")\n",
    "\n",
    "\t# 2. Find and Sort NetCDF Files\n",
    "\tnc_input_dir = pathlib.Path(NC_INPUT_DIRECTORY)\n",
    "\tall_nc_files = glob.glob(str(nc_input_dir / NC_FILE_PATTERN))\n",
    "\tall_nc_files.sort() # Ensure base list is chronological\n",
    "\n",
    "\tif not all_nc_files:\n",
    "\t\traise FileNotFoundError(f\"No NetCDF files found matching pattern '{NC_FILE_PATTERN}' in directory: {NC_INPUT_DIRECTORY}\")\n",
    "\n",
    "\t# 3. Filter NC Files based on USER selection\n",
    "\tif NC_FILES_TO_PROCESS:\n",
    "\t\tnc_files_to_filter = set(NC_FILES_TO_PROCESS)\n",
    "\t\tnc_files = [f for f in all_nc_files if pathlib.Path(f).name in nc_files_to_filter]\n",
    "\t\t\n",
    "\t\tif len(nc_files) != len(NC_FILES_TO_PROCESS):\n",
    "\t\t\tmissing = nc_files_to_filter - set(pathlib.Path(f).name for f in nc_files)\n",
    "\t\t\tprint(f\"⚠️ Warning: {len(missing)} specified NC files were not found: {missing}\")\n",
    "\telse:\n",
    "\t\tnc_files = all_nc_files\n",
    "\n",
    "\tif not nc_files:\n",
    "\t\traise ValueError(\"After filtering, no NetCDF files remain to be processed.\")\n",
    "\t\n",
    "\t# Re-sort to guarantee chronological order for merging phase\n",
    "\tnc_files.sort() \n",
    "\t\n",
    "\tprint(f\"Found {len(nc_files)} NetCDF files for processing (from a total of {len(all_nc_files)}).\")\n",
    "\n",
    "\t# 4. Filter Points for Smart Resume (based on final merged file)\n",
    "\tif not OVERWRITE_EXISTING:\n",
    "\t\tprocessed_ids = []\n",
    "\t\tif pathlib.Path(FINAL_MERGED_OUTPUT_DIR).exists():\n",
    "\t\t\tfor file_path in pathlib.Path(FINAL_MERGED_OUTPUT_DIR).glob(\"*.csv\"):\n",
    "\t\t\t\tpoint_id_from_file = file_path.stem\n",
    "\t\t\t\t# Quick check to ensure file is not just a header\n",
    "\t\t\t\tif file_path.stat().st_size > 1024: \n",
    "\t\t\t\t\tprocessed_ids.append(point_id_from_file)\n",
    "\n",
    "\t\tprocessed_set = set(processed_ids)\n",
    "\t\tpoints_to_process = points_df[~points_df[POINT_ID_FIELD].astype(str).isin(processed_set)].copy()\n",
    "\n",
    "\t\tif len(points_to_process) < len(points_df):\n",
    "\t\t\tskipped_count = len(points_df) - len(points_to_process)\n",
    "\t\t\tprint(f\"Smart Resume Enabled: Skipping {skipped_count}/{len(points_df)} points that are already merged in the final directory.\")\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Smart Resume: No existing merged files found for skipping.\")\n",
    "\telse:\n",
    "\t\tpoints_to_process = points_df.copy()\n",
    "\n",
    "\treturn points_to_process, nc_files\n",
    "\n",
    "def run_extraction_phase(points_to_process: pd.DataFrame, nc_files: List[str]) -> bool:\n",
    "\t\"\"\"Phase 1: Extracts time-series data using a multiprocessing pool.\"\"\"\n",
    "\tprint(\"\\n\" + \"=\"*80)\n",
    "\tprint(f\"PHASE 1: PARALLEL DATA EXTRACTION (Using {CPU_CORES_TO_USE} cores)\")\n",
    "\tprint(\"=\"*80)\n",
    "\n",
    "\textraction_tasks = []\n",
    "\tfor index, point_data in points_to_process.iterrows():\n",
    "\t\tfor nc_file_path in nc_files:\n",
    "\t\t\textraction_tasks.append((point_data, nc_file_path))\n",
    "\n",
    "\ttotal_tasks = len(extraction_tasks)\n",
    "\tif total_tasks == 0:\n",
    "\t\tprint(\"Sub-Process: No points or NC files to process. Skipping Phase 1.\")\n",
    "\t\treturn True\n",
    "\n",
    "\tprint(f\"Sub-Process: Initializing {total_tasks} extraction tasks for parallel execution...\")\n",
    "\t# ADDED FEEDBACK: Inform the user that the pool is starting up.\n",
    "\tprint(\"Sub-Process: Waiting for the first worker to return a result (this may take a moment)...\")\n",
    "\n",
    "\tfailed_tasks = 0\n",
    "\tverified_skip_count = 0\n",
    "\tprocessed_count = 0\n",
    "\t\n",
    "\t# --- Execute in Parallel ---\n",
    "\tprint(\"\\n✅ REAL-TIME EXTRACTION/VERIFICATION REPORT\")\n",
    "\tprint(REPORT_SEPARATOR)\n",
    "\tprint(REPORT_HEADER)\n",
    "\tprint(REPORT_SEPARATOR)\n",
    "\n",
    "\ttry:\n",
    "\t\t# Use a descriptive element for the status bar during startup\n",
    "\t\tinitial_desc = \"Extraction Progress (Awaiting first result)\"\n",
    "\t\t\n",
    "\t\twith mp.Pool(processes=CPU_CORES_TO_USE) as pool:\n",
    "\t\t\t# imap_unordered provides results as soon as they are ready, ideal for real-time reporting\n",
    "\t\t\tresults = pool.imap_unordered(extraction_worker_parallel, extraction_tasks)\n",
    "\t\t\t\n",
    "\t\t\t# Use tqdm to wrap the results iterable for the progress bar\n",
    "\t\t\tfor success, point_id, nc_stem, message in tqdm(results, total=total_tasks, desc=initial_desc):\n",
    "\t\t\t\t\n",
    "\t\t\t\t# After the first result, change the description to a more active state\n",
    "\t\t\t\tif processed_count + verified_skip_count + failed_tasks == 0:\n",
    "\t\t\t\t\t# This block executes only after the first result is processed\n",
    "\t\t\t\t\t# Use a lock to ensure the print and set_description are handled correctly with tqdm\n",
    "\t\t\t\t\ttqdm.set_lock(mp.Lock())\n",
    "\t\t\t\t\ttqdm.get_lock().acquire()\n",
    "\t\t\t\t\ttqdm.write(\"Sub-Process: First result received. Extraction in progress.\")\n",
    "\t\t\t\t\ttqdm.get_lock().release()\n",
    "\t\t\t\t\ttqdm.tqdm.set_description('Extraction Progress')\n",
    "\n",
    "\t\t\t\t# Report status to the console\n",
    "\t\t\t\tif success:\n",
    "\t\t\t\t\tif message.startswith(\"SKIPPED\"):\n",
    "\t\t\t\t\t\treport_task_result(point_id, nc_stem, \"SKIPPED\", \"verified\")\n",
    "\t\t\t\t\t\tverified_skip_count += 1\n",
    "\t\t\t\t\telif message.startswith(\"PROCESSED\"):\n",
    "\t\t\t\t\t\treport_task_result(point_id, nc_stem, \"SUCCESS\", \"processed\")\n",
    "\t\t\t\t\t\tprocessed_count += 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\treport_task_result(point_id, nc_stem, \"FAILED\", \"error\")\n",
    "\t\t\t\t\tfailed_tasks += 1\n",
    "\t\t\t\t\t\n",
    "\t\t\tprint(REPORT_SEPARATOR) # Footer line\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"\\nFATAL: An error occurred during parallel execution: {e}\")\n",
    "\t\treturn False\n",
    "\t\t\n",
    "\tif failed_tasks > 0:\n",
    "\t\tprint(f\"\\n⚠️ Phase 1 completed with {failed_tasks} failed tasks. Check the report above for details.\")\n",
    "\t\n",
    "\tprint(f\"\\nSummary: {processed_count} tasks completed, {verified_skip_count} tasks verified and skipped, {failed_tasks} failed.\")\n",
    "\n",
    "\treturn failed_tasks == 0\n",
    "\n",
    "def run_merging_phase(points_df: pd.DataFrame) -> bool:\n",
    "\t\"\"\"Phase 2: Merges yearly CSV files for each point sequentially.\"\"\"\n",
    "\tprint(\"\\n\" + \"=\"*80)\n",
    "\tprint(\"PHASE 2: SEQUENTIAL TIME-SERIES MERGING\")\n",
    "\tprint(\"=\"*80)\n",
    "\n",
    "\tyearly_data_path = pathlib.Path(YEARLY_DATA_DIR)\n",
    "\tif not yearly_data_path.exists():\n",
    "\t\tprint(\"Sub-Process: Intermediate yearly data directory does not exist. Skipping Phase 2.\")\n",
    "\t\treturn True\n",
    "\n",
    "\t# Collect all NC file stems that actually produced output directories\n",
    "\tnc_file_stems = [p.name for p in yearly_data_path.iterdir() if p.is_dir()]\n",
    "\tnc_file_stems.sort()\n",
    "\t\n",
    "\tunique_point_ids = points_df[POINT_ID_FIELD].astype(str).unique()\n",
    "\t# Only merge files for points that exist in the original input\n",
    "\tmerging_tasks = [(str(point_id), nc_file_stems) for point_id in unique_point_ids]\n",
    "\ttotal_tasks = len(merging_tasks)\n",
    "\n",
    "\tif total_tasks == 0:\n",
    "\t\tprint(\"Sub-Process: No points require merging. Skipping Phase 2.\")\n",
    "\t\treturn True\n",
    "\n",
    "\tprint(f\"Sub-Process: Initializing {total_tasks} merging tasks for sequential execution...\")\n",
    "\n",
    "\tsuccessful_merges = 0\n",
    "\tfailed_merges = 0\n",
    "\tfor task in tqdm(merging_tasks, total=total_tasks, desc=\"Merging Progress\"):\n",
    "\t\tsuccess, point_id, message = merge_worker_sequential(*task)\n",
    "\t\tif success:\n",
    "\t\t\tif not message.startswith(\"Skipped\"):\n",
    "\t\t\t\tsuccessful_merges += 1\n",
    "\t\telse:\n",
    "\t\t\tfailed_merges += 1\n",
    "\t\t\tprint(f\"❌ Failed to merge data for Point ID {point_id}. Details: {message}\")\n",
    "\n",
    "\tif failed_merges > 0:\n",
    "\t\tprint(f\"\\n⚠️ Phase 2 completed with {failed_merges} failed merge tasks.\")\n",
    "\telse:\n",
    "\t\tprint(f\"\\n✅ Phase 2 successfully completed. Merged {successful_merges} new files.\")\n",
    "\n",
    "\treturn failed_merges == 0\n",
    "\n",
    "# ================================================================================================\n",
    "# \tSTEP 6: MAIN EXECUTION FLOW & VERIFICATION\n",
    "# ================================================================================================\n",
    "\n",
    "def run_verification_phase(points_df: pd.DataFrame, nc_files: List[str]):\n",
    "\t\"\"\"Final verification and reporting.\"\"\"\n",
    "\tprint(\"\\n\" + \"=\"*80)\n",
    "\tprint(\"PHASE 3: FINAL VERIFICATION & REPORTING\")\n",
    "\tprint(\"=\"*80)\n",
    "\n",
    "\ttotal_input_points = len(points_df)\n",
    "\tfinal_output_dir = pathlib.Path(FINAL_MERGED_OUTPUT_DIR)\n",
    "\n",
    "\tif not final_output_dir.exists():\n",
    "\t\tprint(\"Sub-Process: Final output directory does not exist. Cannot run verification.\")\n",
    "\t\treturn\n",
    "\n",
    "\tall_final_files = list(final_output_dir.glob(\"*.csv\"))\n",
    "\tfinal_file_count = len(all_final_files)\n",
    "\n",
    "\t# 1. Date Range Check (Uses direct open_dataset to ensure file closure)\n",
    "\ttry:\n",
    "\t\tif not nc_files: raise ValueError(\"No NC files were processed.\")\n",
    "\t\t\n",
    "\t\t# CRITICAL FIX: Do NOT use the global 'worker_datasets' cache in the main thread/verification phase\n",
    "\t\twith xr.open_dataset(nc_files[0], engine='netcdf4') as ds_start:\n",
    "\t\t\texpected_start_date = pd.to_datetime(ds_start['time'].values[0])\n",
    "\t\twith xr.open_dataset(nc_files[-1], engine='netcdf4') as ds_end:\n",
    "\t\t\texpected_end_date = pd.to_datetime(ds_end['time'].values[-1])\n",
    "\t\t\n",
    "\t\tprint(\"Sub-Process: Date Range Check...\")\n",
    "\t\tprint(f\"\t- Input NC files define a span from: {expected_start_date.strftime('%Y-%m-%d')} to {expected_end_date.strftime('%Y-%m-%d')}\")\n",
    "\t\tif all_final_files:\n",
    "\t\t\tsample_file = pd.read_csv(all_final_files[0], parse_dates=[OUTPUT_COLUMN_NAMES[\"time\"]])\n",
    "\t\t\tactual_start_date = sample_file[OUTPUT_COLUMN_NAMES[\"time\"]].min()\n",
    "\t\t\tactual_end_date = sample_file[OUTPUT_COLUMN_NAMES[\"time\"]].max()\n",
    "\t\t\tdate_ok = (actual_start_date == expected_start_date) and (actual_end_date == expected_end_date)\n",
    "\t\t\tstatus = \"✅ OK\" if date_ok else \"❌ ERROR\"\n",
    "\t\t\tprint(f\"\t- Sample file '{all_final_files[0].name}' Date Status: {status}\")\n",
    "\t\t\tprint(f\"\t- Sample File Range: {actual_start_date.strftime('%Y-%m-%d')} to {actual_end_date.strftime('%Y-%m-%d')}\")\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"⚠️ Warning: Could not perform full Date Range Check: {e}\")\n",
    "\n",
    "\t# 2. File Count Check\n",
    "\tprint(\"\\nSub-Process: File Count Check (Completeness)...\")\n",
    "\tif final_file_count == total_input_points:\n",
    "\t\tstatus = \"✅ COMPLETE\"\n",
    "\t\tmessage = f\"All {total_input_points} input points have a final merged file.\"\n",
    "\telif final_file_count < total_input_points:\n",
    "\t\tstatus = \"⚠️ PARTIAL\"\n",
    "\t\tmessage = f\"Found {final_file_count}/{total_input_points} final merged files. {total_input_points - final_file_count} points are missing.\"\n",
    "\telse:\n",
    "\t\tstatus = \"❌ ERROR\"\n",
    "\t\tmessage = f\"Found {final_file_count} files for {total_input_points} points (Check for duplicate Point IDs).\"\n",
    "\tprint(f\"\t- Completeness Status: {status}\")\n",
    "\tprint(f\"\t- {message}\")\n",
    "\tprint(f\"\t- Final Output Directory: {FINAL_MERGED_OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "\t\"\"\"The main control function for the entire workflow.\"\"\"\n",
    "\n",
    "\toverall_start_time = time.time()\n",
    "\tprint(\"\\n\" + \"#\"*80)\n",
    "\tprint(\"\t \t \tUNIFIED NETCDF TIME-SERIES EXTRACTION AND MERGING WORKFLOW\")\n",
    "\tprint(\"#\"*80)\n",
    "\t\n",
    "\tif sys.platform.startswith('win'):\n",
    "\t\t# Essential for multiprocessing on Windows\n",
    "\t\tmp.freeze_support() \n",
    "\n",
    "\ttry:\n",
    "\t\t# --- Task 1: Initialization ---\n",
    "\t\tprint(\"\\n\" + \"--- [ Task 1: Initialization and File Discovery ] ---\")\n",
    "\t\tpoints_df = pd.read_csv(POINT_CSV_FILE_PATH)\n",
    "\t\tpoints_to_process, nc_files = prepare_data_and_files(points_df)\n",
    "\n",
    "\t\t# --- Task 2: Run Extraction (Phase 1) ---\n",
    "\t\textraction_successful = run_extraction_phase(points_to_process, nc_files)\n",
    "\n",
    "\t\t# --- Task 3: Run Merging (Phase 2) ---\n",
    "\t\t# Check if any data exists in the temp directory to proceed with merging\n",
    "\t\thas_intermediate_data = any(pathlib.Path(YEARLY_DATA_DIR).rglob('*.csv'))\n",
    "\n",
    "\t\tif extraction_successful or has_intermediate_data:\n",
    "\t\t\trun_merging_phase(points_df)\n",
    "\t\telse:\n",
    "\t\t\tprint(\"\\nSkipping merging phase due to extraction errors and lack of existing intermediate data.\")\n",
    "\n",
    "\t\t# --- Task 4: Verification and Cleanup (Phase 3) ---\n",
    "\t\trun_verification_phase(points_df, nc_files)\n",
    "\n",
    "\t\tif CLEANUP_YEARLY_DIR and pathlib.Path(YEARLY_DATA_DIR).exists():\n",
    "\t\t\tprint(f\"\\n--- [ Task 5: Cleanup ] ---\")\n",
    "\t\t\tshutil.rmtree(YEARLY_DATA_DIR)\n",
    "\t\t\tprint(\"Sub-Process: Yearly data directory removed.\")\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"\\n\\n{'='*80}\\nFATAL ERROR: The workflow stopped unexpectedly.\\nError Details: {e}\\n{'='*80}\")\n",
    "\t\treturn\n",
    "\n",
    "\t# --- FINAL NOTIFICATION ---\n",
    "\toverall_end_time = time.time()\n",
    "\tprint(f\"\\n\\n{'='*80}\")\n",
    "\tprint(\"✅ ALL TASKS COMPLETE\")\n",
    "\tprint(f\"Total elapsed time: {overall_end_time - overall_start_time:.2f} seconds.\")\n",
    "\tprint(f\"Results saved to: {FINAL_MERGED_OUTPUT_DIR}\")\n",
    "\tprint('='*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4004147b-dcf1-469c-b968-d1f51da53f18",
   "metadata": {},
   "source": [
    "Output preview\n",
    "\n",
    "################################################################################\n",
    "\t \t \tUNIFIED NETCDF TIME-SERIES EXTRACTION AND MERGING WORKFLOW\n",
    "################################################################################\n",
    "\n",
    "--- [ Task 1: Initialization and File Discovery ] ---\n",
    "Loaded 415 points from Outlest_subbasin.csv.\n",
    "Found 1 NetCDF files for processing (from a total of 73).\n",
    "Smart Resume: No existing merged files found for skipping.\n",
    "\n",
    "================================================================================\n",
    "PHASE 1: PARALLEL DATA EXTRACTION (Using 6 cores)\n",
    "================================================================================\n",
    "Sub-Process: Initializing 415 extraction tasks for parallel execution...\n",
    "Sub-Process: Waiting for the first worker to return a result (this may take a moment)...\n",
    "\n",
    "✅ REAL-TIME EXTRACTION/VERIFICATION REPORT\n",
    "---------------------------------------------------------------------------\n",
    "| Point ID & File Stem \t \t \t \t \t \t | Status\t | Action\t \n",
    "---------------------------------------------------------------------------\n",
    "\n",
    "Extraction Progress (Awaiting first result):   0%|                                             | 0/415 [00:00<?, ?it/s]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
